{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copia de 2b - bot_dnn_spacy_esp.ipynb","provenance":[{"file_id":"17zPSAGwxWM9Ta5NhdmtPeYsrNqT56sil","timestamp":1647210633772}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"NV8wZ0MTKjv_"},"source":["<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Procesamiento de lenguaje natural\n","## Bot basado en reglas con DNN + Spacy"]},{"cell_type":"markdown","metadata":{"id":"Z_9z3H-yKrcK"},"source":["#### Datos\n","Este ejemplo se inspiró en otro Bot en inglés creado con NLTK, lo tienen como referencia para hacer lo mismo en inglés:\\\n","[LINK](https://towardsdatascience.com/a-simple-chatbot-in-python-with-deep-learning-3e8669997758)"]},{"cell_type":"markdown","metadata":{"id":"oCVZakCzAjGN"},"source":["### 1 - Instalar dependencias\n","Para poder utilizar Spacy en castellano es necesario agregar la librería \"spacy-stanza\" para lematizar palabras en español."]},{"cell_type":"code","metadata":{"id":"Zd8NLa4gsSmT"},"source":["# La última versión de spacy-stanza (>1.0) es compatible solo con spacy >=3.0\n","# Nota: spacy 3.0 incorpora al pepiline nlp transformers\n","!pip install -U spacy==3.1 --quiet\n","!pip install -U spacy-stanza==1.0.0 --quiet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kzao7XO9NJAq"},"source":["import json\n","import string\n","import random \n","import numpy as np\n","\n","import tensorflow as tf \n","from tensorflow.keras import Sequential \n","from tensorflow.keras.layers import Dense, Dropout"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z_ExOb8uvjqK"},"source":["import stanza\n","import spacy_stanza\n","\n","# Descargar el diccionario en español y armar el pipeline de NLP con spacy\n","stanza.download(\"es\")\n","nlp = spacy_stanza.load_pipeline(\"es\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_wF10RjVMBdV"},"source":["### 2 - Herramientas de preprocesamiento de datos\n","Entre las tareas de procesamiento de texto en español se implementa:\n","- Quitar acentos y caracteres especiales\n","- Quitar números\n","- Quitar símbolos de puntuación"]},{"cell_type":"code","metadata":{"id":"ZxoD2hEExmuX"},"source":["import re\n","import unicodedata\n","\n","# El preprocesamento en castellano requiere más trabajo\n","\n","def preprocess_clean_text(text):    \n","    # sacar tildes de las palabras\n","    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n","    # quitar caracteres especiales\n","    pattern = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]' \n","    text = re.sub(pattern, '', text)\n","    pattern = r'[^a-zA-z.,!?/:;\\\"\\'\\s]' \n","    # quitar números\n","    text = re.sub(pattern, '', text)\n","    # quitar caracteres de puntiación\n","    text = ''.join([c for c in text if c not in string.punctuation])\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q-MiMZjh5fu2"},"source":["text = \"personas Ideas! estás cosas y los peces y los muercielagos\"\n","\n","# Antes de preprocesar los datos se pasa a mínusculas todo el texto\n","preprocess_clean_text(text.lower())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I9V-S8JbrtNn"},"source":["# Ejemplo de como fuciona\n","text = \"hola personas Ideas! estás cosas y los peces y los muercielagos\"\n","\n","# Antes de preprocesar los datos se pasa a mínusculas todo el texto\n","tokes = nlp(preprocess_clean_text(text.lower()))\n","print(\"tokens:\", tokes)\n","print(\"Lematización de cada token:\")\n","for token in tokes:\n","    print([token, token.lemma_])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ilRbn0KfMm2r"},"source":["### 3 - Diccionario de entrada"]},{"cell_type":"code","metadata":{"id":"NgIGpjymNEH7"},"source":["# Dataset en formato JSON que representa las posibles preguntas (patterns)\n","# y las posibles respuestas por categoría (tag)\n","dataset = {\"intents\": [\n","             {\"tag\": \"bienvenida\",\n","              \"patterns\": [\"Hola\", \"¿Cómo estás?\", \"¿Qué tal?\"],\n","              \"responses\": [\"Hola!\", \"Hola, ¿Cómo estás?\"],\n","             },\n","             {\"tag\": \"nombre\",\n","              \"patterns\": [\"¿Cúal es tu nombre?\", \"¿Quién sos?\"],\n","              \"responses\": [\"Mi nombre es TiendaPro\", \"Yo soy TiendaPro\"]\n","             },\n","            {\"tag\": \"contacto\",\n","              \"patterns\": [\"contacto\", \"número de contacto\", \"número de teléfono\", \"número de whatsapp\", \"whatsapp\"],\n","              \"responses\": [\"Podes contactarnos al siguiente <numero>\", \"Contactos al whatsapp <numero>\"]\n","             },\n","            {\"tag\": \"envios\",\n","              \"patterns\": [\"¿Realizan envios?\", \"¿Cómo me llega el paquete?\"],\n","              \"responses\": [\"Tenemos diferentes formas de envios según la zona, te recomiendo entrar a este <link>\"]\n","             },\n","            {\"tag\": \"precios\",\n","              \"patterns\": [\"precio\", \"Me podrás pasar los precios\", \"¿Cuánto vale?\", \"¿Cuánto sale?\"],\n","              \"responses\": [\"En el siguiente link podrás encontrar los precios de todos nuestros productos en stock\"]\n","             },\n","            {\"tag\": \"pagos\",\n","              \"patterns\": [\"medios de pago\", \"tarjeta de crédito\", \"tarjetas\", \"cuotas\"],\n","              \"responses\": [\"En el siguiente link podrás encontrar los beneficios y formas de pago vigentes\"]\n","             },\n","            {\"tag\": \"stock\",\n","              \"patterns\": [\"Esto está disponible\", \"¿Tenes stock?\", \"¿Hay stock hoy?\"],\n","              \"responses\": [\"Los productos publicados están en stock\"]\n","             },\n","            {\"tag\": \"agradecimientos\",\n","              \"patterns\": [ \"Muchas gracias\", \"Gracias\"],\n","              \"responses\": [\"Por nada!, cualquier otra consulta podes escribirme\"]\n","             },\n","             {\"tag\": \"despedida\",\n","              \"patterns\": [ \"Chau\", \"Hasta luego!\"],\n","              \"responses\": [\"Hasta luego!\", \"Hablamos luego!\"]\n","             }\n","]}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"19PEDmIDfLRu"},"source":["### 4 - Preprocesamiento y armado del dataset"]},{"cell_type":"code","metadata":{"id":"b3HP8abHNRk3"},"source":["# Datos que necesitaremos, las palabras o vocabilario\n","words = []\n","classes = []\n","doc_X = []\n","doc_y = []\n","\n","# Por cada intención (intents) debemos tomar los patrones que la caracterízan\n","# a esa intención y transformarla a tokens para lamacenar en doc_X\n","\n","# El tag de cada intención se almacena como doc_Y (la clase a predecir)\n","\n","for intent in dataset[\"intents\"]:\n","    for pattern in intent[\"patterns\"]:\n","        # trasformar el patron a tokens\n","        tokens = nlp(preprocess_clean_text(pattern.lower()))\n","        # lematizar los tokens\n","        for token in tokens:            \n","            words.append(token.lemma_)\n","        \n","        doc_X.append(pattern)\n","        doc_y.append(intent[\"tag\"])\n","    \n","    # Agregar el tag a las clases\n","    if intent[\"tag\"] not in classes:\n","        classes.append(intent[\"tag\"])\n","\n","# Elminar duplicados con \"set\" y ordenar el vocubulario y las clases por orden alfabético\n","words = sorted(set(words))\n","classes = sorted(set(classes))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Acy-gcugNbMH"},"source":["print(\"words:\", words)\n","print(\"classes:\", classes)\n","print(\"doc_X:\", doc_X)\n","print(\"doc_y:\", doc_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YI0L2U7IQcvy"},"source":["# Tamaño del vocabulario\n","print(\"Vocabulario:\", len(words))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hqBeGKRk_q4r"},"source":["# Cantidad de tags\n","print(\"Tags:\", len(classes))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vpbJ0guPN2Uq"},"source":["# Transformar doc_X en bag of words por oneHotEncoding\n","# Transformar doc_Y en un vector de clases multicategórico con oneHotEncoding\n","\n","training = []\n","out_empty = [0] * len(classes)\n","\n","for idx, doc in enumerate(doc_X):\n","    # Transformar la pregunta (input) en tokens y lematizar\n","    text = []\n","    tokens = nlp(preprocess_clean_text(doc.lower()))\n","    for token in tokens:\n","        text.append(token.lemma_)\n","\n","    # Transformar los tokens en \"Bag of words\" (arrays de 1 y 0)\n","    bow = []\n","    for word in words:\n","        bow.append(1) if word in text else bow.append(0)\n","    \n","    # Crear el array de salida (class output) correspondiente\n","    output_row = list(out_empty)\n","    output_row[classes.index(doc_y[idx])] = 1\n","\n","    print(\"X:\", bow, \"y:\", output_row)\n","    training.append([bow, output_row])\n","\n","# Mezclar los datos\n","random.shuffle(training)\n","training = np.array(training, dtype=object)\n","# Dividir en datos de entrada y salida\n","train_X = np.array(list(training[:, 0]))\n","train_y = np.array(list(training[:, 1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a_Hr8QaDfRf3"},"source":["### 5 - Entrenamiento del modelo"]},{"cell_type":"code","metadata":{"id":"fopb3NqcAGTz"},"source":["# Shape de entrada y salida\n","input_shape = (train_X.shape[1],)\n","output_shape = train_y.shape[1]\n","print(\"input:\", input_shape, \"output:\", output_shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xy7tzkwdOZx9"},"source":["# Entrenamiento del modelo DNN\n","# - Modelo secuencial\n","# - Con regularización\n","# - softmax y optimizador Adam\n","model = Sequential()\n","model.add(Dense(128, input_shape=input_shape, activation=\"relu\"))\n","model.add(Dropout(0.5))\n","model.add(Dense(64, activation=\"relu\"))\n","model.add(Dropout(0.5))\n","model.add(Dense(output_shape, activation = \"softmax\"))\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=\"Adam\",\n","              metrics=[\"accuracy\"])\n","print(model.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r6hi4EcdOghm"},"source":["hist = model.fit(x=train_X, y=train_y, epochs=200)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pb1GZDjGRP6Q"},"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Entrenamiento\n","epoch_count = range(1, len(hist.history['accuracy']) + 1)\n","sns.lineplot(x=epoch_count,  y=hist.history['accuracy'], label='train')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eTVDnrV0mDRf"},"source":["# Guardar lo necesario para poder re-utilizar este modelo en el futuro\n","# el vocabulario utilizado (words)\n","# las posibles clases\n","# el modelo\n","import pickle\n","pickle.dump(words, open('words.pkl','wb'))\n","pickle.dump(classes, open('classes.pkl','wb'))\n","model.save('chatbot_model.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TnD1WvhBfVYR"},"source":["### 6 - Testing y validación"]},{"cell_type":"code","metadata":{"id":"kqBdSGt8Orkm"},"source":["def text_to_tokens(text): \n","    lemma_tokens = []\n","    tokens = nlp(preprocess_clean_text(text.lower()))\n","    for token in tokens:\n","        lemma_tokens.append(token.lemma_)\n","    #print(lemma_tokens)\n","    return lemma_tokens\n","\n","def bag_of_words(text, vocab): \n","    tokens = text_to_tokens(text)\n","    bow = [0] * len(vocab)\n","    for w in tokens: \n","        for idx, word in enumerate(vocab):\n","            if word == w: \n","                bow[idx] = 1\n","    #print(bow)\n","    return np.array(bow)\n","\n","def pred_class(text, vocab, labels): \n","    bow = bag_of_words(text, vocab)\n","    words_recognized = sum(bow)\n","\n","    return_list = []\n","    if words_recognized > 0:\n","        result = model.predict(np.array([bow]))[0]\n","        thresh = 0.2\n","        y_pred = [[idx, res] for idx, res in enumerate(result) if res > thresh]\n","        y_pred.sort(key=lambda x: x[1], reverse=True)\n","    \n","        for r in y_pred:\n","            return_list.append(labels[r[0]])\n","            #print(labels[r[0]], r[1])\n","\n","    return return_list\n","\n","def get_response(intents_list, intents_json):\n","    tag = intents_list[0]\n","    list_of_intents = intents_json[\"intents\"]\n","    for i in list_of_intents: \n","        if i[\"tag\"] == tag:\n","            result = \"BOT: \" + random.choice(i[\"responses\"])\n","            break\n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xp1vXQwdOvl7"},"source":["while True:\n","    message = input(\"\")\n","    intents = pred_class(message, words, classes)\n","    if len(intents) > 0:\n","        result = get_response(intents, dataset)\n","        print(result)\n","    else:\n","        print(\"Perdón, no comprendo la pregunta.\")\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ayatkwp4fYQx"},"source":["### 7 - Conclusiones\n","El bot tal cual está definido es capaz de responder a bastantes tipos de preguntas con gran precisión. Algunas técnicas que podrían ensayarse para evaluar como impactan en el sistema son:\n","- Filtrar los stop words\n","- Utilizar TF-IDF en vez de bag of words"]}]}