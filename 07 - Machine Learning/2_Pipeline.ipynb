{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Train-Test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Normalization\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# Feature selection\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Classification models\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_skewed_boundaries(df, variable, distance=1.5):\n",
    "    IQR = df[variable].quantile(0.75) - df[variable].quantile(0.25)\n",
    "    lower_boundary = df[variable].quantile(0.25) - (IQR * distance)\n",
    "    upper_boundary = df[variable].quantile(0.75) + (IQR * distance)\n",
    "    return upper_boundary, lower_boundary\n",
    "    \n",
    "def transform_outliers(df, norm_col, threshold_capped=1.5, threshold_trimmed=1.8,  test_mode=False, limits={},\n",
    "                        use_manual_limits=False, upper_limit_trim=0, lower_limit_trim=0, upper_limit_cap=0, lower_limit_cap=0, print_output=False):\n",
    "    \n",
    "    outliers_limits={}\n",
    "    # Trimming and capping outliers\n",
    "    outliers_total = np.array(np.repeat(False,df.shape[0]))\n",
    "    df_capped = df.copy()\n",
    "    for col in norm_col:\n",
    "        \n",
    "        if not test_mode:\n",
    "            outliers_limits_col={}\n",
    "            if use_manual_limits:\n",
    "                upper_limit, lower_limit = upper_limit_trim, lower_limit_trim\n",
    "            else:\n",
    "                upper_limit, lower_limit = find_skewed_boundaries(df, col, threshold_trimmed)\n",
    "\n",
    "            outliers_limits_col['upper_limit_trim'] = upper_limit\n",
    "            outliers_limits_col['lower_limit_trim'] = lower_limit\n",
    "        else:\n",
    "            upper_limit, lower_limit = limits[col]['upper_limit_trim'], limits[col]['lower_limit_trim']\n",
    "\n",
    "        outliers = np.where(df[col] > upper_limit, True,\n",
    "                            np.where(df[col] < lower_limit, True, False))                        \n",
    "        outliers_total = np.logical_or(outliers_total, outliers)\n",
    "        \n",
    "        if print_output:\n",
    "            print(str(col) + \" outliers = \"+str(outliers.sum()))\n",
    "        \n",
    "        if not test_mode:\n",
    "            if use_manual_limits:\n",
    "                upper_limit, lower_limit = upper_limit_cap, lower_limit_cap\n",
    "            else:\n",
    "                upper_limit, lower_limit = find_skewed_boundaries(df, col, threshold_capped)\n",
    "\n",
    "            outliers_limits_col['upper_limit_cap'] = upper_limit\n",
    "            outliers_limits_col['lower_limit_cap'] = lower_limit\n",
    "            outliers_limits[col] = outliers_limits_col\n",
    "        else:\n",
    "            upper_limit, lower_limit = limits[col]['upper_limit_cap'], limits[col]['lower_limit_cap']\n",
    "\n",
    "        df_capped[col] = np.where(df[col] > upper_limit, upper_limit,\n",
    "                            np.where(df[col] < lower_limit, lower_limit, df_capped[col]))\n",
    "\n",
    "    if print_output:\n",
    "        print(\"Total outliers = \"+str(outliers_total.sum()))\n",
    "        \n",
    "    df_trimmed = df_capped.loc[~(outliers_total)]\n",
    "    \n",
    "    if not test_mode:\n",
    "        return df_trimmed, outliers_limits\n",
    "    else:\n",
    "        return df_trimmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple median imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_imputer(df,cols,test_mode=False,imputer_trained=None):\n",
    "    df_imputed = df.copy()\n",
    "\n",
    "            \n",
    "    if not test_mode:\n",
    "        imputer = SimpleImputer(strategy='median',add_indicator=True) # Create imputer\n",
    "    else:\n",
    "        imputer = imputer_trained\n",
    "\n",
    "    X_imputed_median = imputer.fit_transform(df_imputed[cols]) # Fit-transform\n",
    "    imputed_median_cols = [str(d) + \"_imputed\" for d in cols]  # Name of indicators of imputation\n",
    "    median_cols = cols + imputed_median_cols\n",
    "\n",
    "    assert(not np.isnan(np.sum(X_imputed_median))) # Check not nan\n",
    "\n",
    "    # Replace in dataset\n",
    "    df_imputed.drop(cols,axis=1,inplace=True)\n",
    "    df_imputed[median_cols]=X_imputed_median\n",
    "    \n",
    "    if not test_mode:\n",
    "        return df_imputed, imputer\n",
    "    else:\n",
    "        return df_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_imputer(df, imp_cols, neighbors_cols, neighbors=5,test_mode=False,imputer_trained=None):\n",
    "    df_imputed = df.copy()\n",
    "\n",
    "    if not test_mode:            \n",
    "        imputer = KNNImputer(n_neighbors=neighbors,add_indicator=True)\n",
    "    else:\n",
    "        imputer = imputer_trained\n",
    "\n",
    "    cols = imp_cols+neighbors_cols\n",
    "    X_imputed_knn = imputer.fit_transform(df_imputed[cols]) # Fit-transform\n",
    "    imputed_knn_cols = [str(d) + \"_imputed\" for d in imp_cols] # Name of indicators of imputation\n",
    "    knn_cols = cols + imputed_knn_cols\n",
    "\n",
    "    assert(not np.isnan(np.sum(X_imputed_knn))) # Check not nan\n",
    "    \n",
    "    # Replace in dataset\n",
    "    df_imputed.drop(cols,axis=1,inplace=True)\n",
    "    df_imputed[knn_cols]=X_imputed_knn\n",
    "    \n",
    "    if not test_mode:\n",
    "        return df_imputed, imputer\n",
    "    else:\n",
    "        return df_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape, X_test.shape = \n",
      "(2784, 9) (492, 9)\n",
      "% of Potability in original dataset: 39.010989010989015\n",
      "% of Potability in y_train: 39.008620689655174\n",
      "% of Potability in y_test: 39.02439024390244\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('water_potability.csv')\n",
    "df = df[df['Potability'].notna()]\n",
    "\n",
    "y = df['Potability']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(['Potability'], axis=1),\n",
    "    y,\n",
    "    test_size=0.15,\n",
    "    random_state=0,\n",
    "    stratify=y\n",
    "    )\n",
    "    \n",
    "print(\"X_train.shape, X_test.shape = \")\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "print(\"% of Potability in original dataset: \"+str((df['Potability']==1).sum()/df.shape[0]*100))\n",
    "print(\"% of Potability in y_train: \"+str((y_train==1).sum()/y_train.shape[0]*100))\n",
    "print(\"% of Potability in y_test: \"+str((y_test==1).sum()/y_test.shape[0]*100))\n",
    "\n",
    "# Create df_train and df_test to process all columns together\n",
    "df_train = X_train.copy()\n",
    "df_train['Potability']=y_train\n",
    "\n",
    "df_test = X_test.copy()\n",
    "df_test['Potability']=y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_df(df, test_mode=False, param_dict={}):\n",
    "\n",
    "    if test_mode and not param_dict:\n",
    "        raise Exception('param_list need for test mode')\n",
    "\n",
    "    df_ = df.copy()\n",
    "\n",
    "    # Outliers\n",
    "    norm_col = [\n",
    "                'ph', \n",
    "                'Hardness', \n",
    "                'Solids', \n",
    "                'Chloramines',\n",
    "                'Sulfate', \n",
    "                'Conductivity',\n",
    "                'Organic_carbon', \n",
    "                'Trihalomethanes', \n",
    "                'Turbidity'\n",
    "            ]\n",
    "\n",
    "    if not test_mode:\n",
    "        df_, outliers_limits_ = transform_outliers(df_, norm_col)\n",
    "    else:\n",
    "        df_ = transform_outliers(df_, norm_col, test_mode=test_mode, limits=param_dict['outliers_limits'])\n",
    "    \n",
    "    # Scaling - General\n",
    "    scaled_columns = [\n",
    "                'ph', \n",
    "                'Hardness', \n",
    "                'Solids', \n",
    "                'Chloramines',\n",
    "                'Sulfate', \n",
    "                'Conductivity',\n",
    "                'Organic_carbon', \n",
    "                'Trihalomethanes', \n",
    "                'Turbidity'\n",
    "            ]\n",
    "\n",
    "    if not test_mode:\n",
    "        scaler_ = StandardScaler()\n",
    "        scaler_.fit(df_[scaled_columns])\n",
    "    else:\n",
    "        scaler_ = param_dict['scaler']\n",
    "        \n",
    "    df_[scaled_columns] = scaler_.transform(df_[scaled_columns])\n",
    "\n",
    "\n",
    "    # Imputation\n",
    "    imputed_by_median_col = ['Trihalomethanes']           \n",
    "    imputed_by_knn_col = ['ph','Sulfate']\n",
    "    neighbors_col =['Hardness','Solids','Chloramines','Conductivity','Organic_carbon','Trihalomethanes','Turbidity']\n",
    "\n",
    "    if not test_mode:\n",
    "        df_, median_imputer_list_ = median_imputer(df_,imputed_by_median_col)\n",
    "        df_, knn_imputer_list_ = knn_imputer(df_, imputed_by_knn_col, neighbors_col)\n",
    "    else:\n",
    "        df_ = median_imputer(df_, imputed_by_median_col, test_mode=test_mode, imputer_trained=param_dict['median_imputer_list'])\n",
    "        df_ = knn_imputer(df_, imputed_by_knn_col, neighbors_col, test_mode=test_mode, imputer_trained=param_dict['knn_imputer_list'])\n",
    "\n",
    "    # Normalization\n",
    "    norm_col_yj = [\n",
    "        'Hardness',\n",
    "        'Solids',\n",
    "        'Chloramines',\n",
    "        'Conductivity',\n",
    "        'Organic_carbon',\n",
    "        'Turbidity',\n",
    "        'Trihalomethanes',\n",
    "        'ph',\n",
    "        'Sulfate'\n",
    "    ]\n",
    "    \n",
    "    if not test_mode:\n",
    "        power_yj_ = PowerTransformer(method= 'yeo-johnson')\n",
    "        power_yj_.fit(df_[norm_col_yj])\n",
    "    else:\n",
    "        power_yj_ = param_dict['power_yj']\n",
    "                \n",
    "    df_[norm_col_yj] = power_yj_.transform(df_[norm_col_yj])\n",
    "\n",
    "    if not test_mode:\n",
    "        param_dict={'outliers_limits': outliers_limits_, \n",
    "                'scaler': scaler_, \n",
    "                'median_imputer_list': median_imputer_list_, \n",
    "                'knn_imputer_list': knn_imputer_list_, \n",
    "                'power_yj': power_yj_\n",
    "        }\n",
    "        return df_, param_dict\n",
    "\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ph_imputed', 'Sulfate_imputed']\n",
      "knn_cols\n",
      "11\n",
      "X_imputed_knn\n",
      "(2683, 11)\n",
      "['ph_imputed', 'Sulfate_imputed']\n",
      "knn_cols\n",
      "11\n",
      "X_imputed_knn\n",
      "(472, 11)\n"
     ]
    }
   ],
   "source": [
    "df_train_processed, param_dict = pre_process_df(df_train)\n",
    "df_test_processed = pre_process_df(df_test, test_mode=True, param_dict=param_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data into pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_processed.to_pickle('./data/df_train_processed.pkl')\n",
    "\n",
    "open_file = open('./data/df_param_dict.pkl', \"wb\")\n",
    "pickle.dump(param_dict, open_file)\n",
    "open_file.close()\n",
    "\n",
    "df_test_processed.to_pickle('./data/df_test_processed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
