{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Implementación de PCA en NumPy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "# import sklearn as skl\r\n",
    "from sklearn.decomposition import PCA"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Objetivos\n",
    "* Implementación de PCA en NumPy paso a paso\n",
    "* Comparación de resultados con Scikit-learn"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementación"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Dado un dataset $X \\in \\mathbb{R}^{n, d}$, con $n$ muestras y $d$ features, queremos reducir sus dimensiones a $m$. Para ello, el primer paso es centrar el dataset (Hint: usen np.mean)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "n = 10\r\n",
    "d = 3\r\n",
    "X = np.random.uniform(low=0., high=100., size=(n,d))\r\n",
    "print(X)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[42.2480501  35.29280919 82.73204353]\n",
      " [43.85261966 53.97444366 39.33641668]\n",
      " [92.05706613 91.62309171 10.6340016 ]\n",
      " [ 9.87765015 58.88238508 89.0634533 ]\n",
      " [ 5.59363733 41.89388889  3.1244253 ]\n",
      " [72.15263047  8.49744244 90.2232056 ]\n",
      " [78.25203309 15.29848102 26.03485004]\n",
      " [38.45827209 45.34578596 54.63736307]\n",
      " [ 7.12248678 94.38150298 25.48554755]\n",
      " [41.56659137 59.06929062 42.94906436]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "X = X-np.mean(X,axis=0)\r\n",
    "print(X)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ -0.87005362 -15.13310296  36.31000643]\n",
      " [  0.73451594   3.54853151  -7.08562042]\n",
      " [ 48.93896242  41.19717956 -35.7880355 ]\n",
      " [-33.24045357   8.45647292  42.6414162 ]\n",
      " [-37.52446639  -8.53202327 -43.2976118 ]\n",
      " [ 29.03452675 -41.92846972  43.80116849]\n",
      " [ 35.13392938 -35.12743113 -20.38718706]\n",
      " [ -4.65983163  -5.08012619   8.21532597]\n",
      " [-35.99561694  43.95559082 -20.93648956]\n",
      " [ -1.55151235   8.64337846  -3.47297274]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Obtener la matriz de covarianza de $X^T$, revisar en la teoría por qué utilizamos la transpuesta. Buscar en la documentación de NumPy qué funciones se pueden utilizar."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "cov = np.cov(X.T)\r\n",
    "print(cov)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 922.94825309 -216.94460388  -33.85925821]\n",
      " [-216.94460388  789.74005209 -381.26398456]\n",
      " [ -33.85925821 -381.26398456 1021.60545611]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Calcular los autovalores y autovectores de la matriz de covarianza. Revisar la documentación de NumPy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "w, v = np.linalg.eig(cov)\r\n",
    "\r\n",
    "print(\"Eigenvalues:\\n\"+str(w))\r\n",
    "print(\"Eigenvectors:\\n\"+str(v))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Eigenvalues:\n",
      "[ 428.77084969  972.78170821 1332.7412034 ]\n",
      "Eigenvectors:\n",
      "[[ 0.3735812   0.88766881  0.26922326]\n",
      " [ 0.77032841 -0.13520131 -0.62314905]\n",
      " [ 0.51675064 -0.4401871   0.73430518]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Ordernar los autovectores en el sentido de los autovalores decrecientes, revisar la teoría de ser necesario."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "indx = w.argsort()[::-1]\r\n",
    "v = v[indx]\r\n",
    "print(v)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 0.51675064 -0.4401871   0.73430518]\n",
      " [ 0.77032841 -0.13520131 -0.62314905]\n",
      " [ 0.3735812   0.88766881  0.26922326]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. Proyectar el dataset centrado sobre los $m$ autovectores más relevantes (Hint: usen np.dot)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "m = 2\r\n",
    "v_reduced = v[:, :m]\r\n",
    "X_reduced = np.dot(X,v_reduced[:, :m])\r\n",
    "print(X_reduced)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[  1.45767571  34.66026212]\n",
      " [  0.46604167  -7.09277484]\n",
      " [ 43.654861   -58.88013567]\n",
      " [  5.267267    51.34014789]\n",
      " [-42.13842563 -20.76261301]\n",
      " [ -0.93178831  31.7690914 ]\n",
      " [-16.52044753 -28.81329775]\n",
      " [ -3.25224522  10.03052615]\n",
      " [  7.43800361  -8.68271631]\n",
      " [  4.5590577   -3.56849   ]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Consolidar los pasos anteriores en una función o clase PCA."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def npPCA(X,m):\r\n",
    "\r\n",
    "    X = X-np.mean(X,axis=0)\r\n",
    "    \r\n",
    "    cov = np.cov(X.T)\r\n",
    "    w, v = np.linalg.eig(cov)\r\n",
    "\r\n",
    "    indx = w.argsort()[::-1]\r\n",
    "    v = v[:, indx]\r\n",
    "    \r\n",
    "    v_reduced = v[:, :m]\r\n",
    "    X_reduced = np.dot(X,v_reduced)\r\n",
    "        \r\n",
    "    return X_reduced"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "n = 10\r\n",
    "d = 3\r\n",
    "X = np.random.uniform(low=0., high=100., size=(n,d))\r\n",
    "print(npPCA(X,2))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 30.64860606 -19.66989077]\n",
      " [ -1.52914288  33.8536157 ]\n",
      " [-45.35675287   6.72044103]\n",
      " [-27.28657471  11.17135254]\n",
      " [-54.03793305 -13.02992229]\n",
      " [-15.77444765  17.73604996]\n",
      " [ 70.03467357  -8.51701226]\n",
      " [-19.04491666 -59.93965952]\n",
      " [-14.83219328  23.36422694]\n",
      " [ 77.17868146   8.31079868]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "7. Comparar los resultados obtenidos con el modelo de PCA implementado en Scikit-learn ([ver documentación](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)). Tomar como dataset:\n",
    "\n",
    "$X=\\begin{bmatrix}\n",
    "0.8 & 0.7\\\\\n",
    "0.1 & -0.1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Se debe reducir a un componente. Verificar los resultados con np.testing.assert_allclose"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "X = np.array([[0.8,0.7],[0.1,-0.1]])\r\n",
    "\r\n",
    "pca = PCA(n_components=1)\r\n",
    "X_new = pca.fit_transform(X)\r\n",
    "print(\"skl result:\\n\"+str(X_new))\r\n",
    "\r\n",
    "myPCA = npPCA(X,1)\r\n",
    "print(\"npPCA result:\\n\"+str(myPCA))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "skl result:\n",
      "[[-0.53150729]\n",
      " [ 0.53150729]]\n",
      "npPCA result:\n",
      "[[-0.53150729]\n",
      " [ 0.53150729]]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}