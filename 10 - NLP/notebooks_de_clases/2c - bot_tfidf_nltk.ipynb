{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copia de 2c - bot_tfidf_nltk.ipynb","provenance":[{"file_id":"14WPcw9uG8ydvgyR3TqRtRrGd36g9oM6y","timestamp":1647211443173}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ue5hxxkdAQJg"},"source":["<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Procesamiento de lenguaje natural\n","## Bot con NLTK utilizando un corpus de wikipedia\n"]},{"cell_type":"code","metadata":{"id":"kCED1hh-Ioyf"},"source":["import json\n","import string\n","import random\n","import re\n","import urllib.request\n","\n","import numpy as np\n","import tensorflow as tf \n","from tensorflow.keras import Sequential \n","from tensorflow.keras.layers import Dense, Dropout\n","\n","# Para leer y parsear el texto en HTML de wikipedia\n","import bs4 as bs\n","\n","import nltk\n","# Descargar el diccionario\n","nltk.download(\"punkt\")\n","nltk.download(\"wordnet\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DMOa4JPSCJ29"},"source":["### Datos\n","Se consumira los datos del artículo de wikipedia sobre el deporte \"tennis\" en ingles."]},{"cell_type":"code","metadata":{"id":"RIO7b8GjAC17"},"source":["raw_html = urllib.request.urlopen('https://en.wikipedia.org/wiki/Tennis')\n","raw_html = raw_html.read()\n","\n","article_html = bs.BeautifulSoup(raw_html, 'lxml')\n","\n","article_paragraphs = article_html.find_all('p')\n","\n","article_text = ''\n","\n","for para in article_paragraphs:\n","    article_text += para.text\n","\n","article_text = article_text.lower()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pUH30a1_rOkS"},"source":["# Demos un vistazo\n","article_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BtGLJjt6rQhK"},"source":["print(\"Cantidad de caracteres en la nota:\", len(article_text))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FVHxBRNzCMOS"},"source":["### 2 - Preprocesamiento\n","- Remover caracteres especiales\n","- Quitar espacios o saltos"]},{"cell_type":"code","metadata":{"id":"HnEUTD1Erl1N"},"source":["text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n","text = re.sub(r'\\s+', ' ', text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g7ycrAMYrn66"},"source":["# Demos un vistazo\n","text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PA5F0s4UsMpf"},"source":["print(\"Cantidad de caracteres en el texto:\", len(text))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DKNcDGcisajf"},"source":["### 3 - Dividir el texto en sentencias y en palabras"]},{"cell_type":"code","metadata":{"id":"reXBOFQ7sdlB"},"source":["corpus = nltk.sent_tokenize(text)\n","words = nltk.word_tokenize(text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J5GloV9fsi6o"},"source":["# Demos un vistazo\n","corpus[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hmQ7nkvvsi0i"},"source":["# Demos un vistazo\n","words[:20]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YXPWNkKfEvDZ"},"source":["print(\"Vocabulario:\", len(words))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NlYKyb3OtDse"},"source":["### 4 - Funciones de ayuda para limpiar y procesar el input del usuario\n","- Lematizar los tokens de la oración\n","- Quitar símbolos de puntuación"]},{"cell_type":"code","metadata":{"id":"afPok8pstPOx"},"source":["from nltk.stem import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","def perform_lemmatization(tokens):\n","    return [lemmatizer.lemmatize(token) for token in tokens]\n","\n","punctuation_removal = dict((ord(punctuation), None) for punctuation in string.punctuation)\n","\n","def get_processed_text(document):\n","    # 1 - reduce el texto a mínuscula\n","    # 2 - quitar los simbolos de puntuacion\n","    # 3 - realiza la tokenización\n","    # 4 - realiza la lematización\n","    return perform_lemmatization(nltk.word_tokenize(document.lower().translate(punctuation_removal)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jl8r6d9ZuyR9"},"source":["### 5 - Utilizar vectores TF-IDF y la similitud coseno construido con el corpus de wikipedia"]},{"cell_type":"code","metadata":{"id":"IRYFHcBfk2Gt"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","def generate_response(user_input, corpus):\n","    response = ''\n","    # Sumar al corpus la pregunta del usuario para calcular\n","    # su cercania con otros documentos/sentencias\n","    corpus.append(user_input)\n","\n","    # Crear un vectorizar TFIDF que quite las \"stop words\" del ingles y utilice\n","    # nuestra funcion para obtener los tokens lematizados \"get_processed_text\"\n","    word_vectorizer = TfidfVectorizer(tokenizer=get_processed_text, stop_words='english')\n","\n","    # Crear los vectores a partir del corpus\n","    all_word_vectors = word_vectorizer.fit_transform(corpus)\n","\n","    # Calcular la similitud coseno entre todas los documentos excepto el agregado (el útlimo \"-1\")\n","    # NOTA: con los word embedings veremos más en detalle esta matriz de similitud\n","    similar_vector_values = cosine_similarity(all_word_vectors[-1], all_word_vectors)\n","\n","    # Obtener el índice del vector más cercano a nuestra oración\n","    # --> descartando la similitud contra nuestor vector propio\n","    similar_sentence_number = similar_vector_values.argsort()[0][-2]\n","    matched_vector = similar_vector_values.flatten()\n","    matched_vector.sort()\n","    vector_matched = matched_vector[-2]\n","\n","    if vector_matched == 0:\n","        response = \"I am sorry, I could not understand you\"\n","    else:\n","        response = corpus[similar_sentence_number]\n","    \n","    corpus.remove(user_input)\n","    return response"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OK-BuXPBybSp"},"source":["### 6 - Ensayar el sistema\n","El sistema intentará encontrar la parte del artículo que más se relaciona con nuestro texto de entrada. Sugerencias ensayar:\n","- Grand slam\n","- tournaments\n","- nadal\n","- artificial intelligence"]},{"cell_type":"code","metadata":{"id":"Z2X4j8XyydSb"},"source":["# Se utilizará gradio para ensayar el bot\n","# Herramienta poderosa para crear interfaces rápidas para ensayar modelos\n","# https://gradio.app/\n","import sys\n","!{sys.executable} -m pip install gradio --quiet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SZv5MiVzynG1"},"source":["import gradio as gr\n","\n","def bot_response(human_text):\n","    print(human_text)\n","    return generate_response(human_text.lower(), corpus)\n","\n","iface = gr.Interface(\n","    fn=bot_response,\n","    inputs=[\"textbox\"],\n","    outputs=\"text\",\n","    layout=\"vertical\")\n","\n","iface.launch(debug=True)"],"execution_count":null,"outputs":[]}]}