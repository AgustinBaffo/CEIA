{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "from geopy.geocoders import Nominatim\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# Train-Test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Location clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "\n",
    "# Scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Normalization\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cols(df,cols):\n",
    "    df_ = df.copy()\n",
    "    df_.drop(cols,axis=1,inplace=True)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GeneralEncoder():\n",
    "#     def __init__(self):\n",
    "#         # Wind direction (ordinal)\n",
    "#         self.windDirDic = {\n",
    "#             \"E\": 0,\n",
    "#             \"ENE\": 1,\n",
    "#             \"NE\": 2,\n",
    "#             \"NNE\": 3,\n",
    "#             \"N\": 4,\n",
    "#             \"NNW\": 5,\n",
    "#             \"NW\": 6,\n",
    "#             \"WNW\": 7,\n",
    "#             \"W\": 8,\n",
    "#             \"WSW\": 9,\n",
    "#             \"SW\": 10,\n",
    "#             \"SSW\": 11,\n",
    "#             \"S\": 12,\n",
    "#             \"SSE\": 13,\n",
    "#             \"SE\": 14,\n",
    "#             \"ESE\": 15,\n",
    "#         }\n",
    "    \n",
    "#     def fit(self):\n",
    "#         pass\n",
    "\n",
    "#     def transform(self, df):\n",
    "#         df_ = df.copy(df)\n",
    "        \n",
    "#         df_.replace({\"No\":0,\"Yes\":1},inplace=True)\n",
    "#         df_.replace(self.windDirDic,inplace=True)\n",
    "#         return df_\n",
    "\n",
    "# from sklearn import set_config\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# set_config(display='diagram')\n",
    "# myPipeline = Pipeline(steps=[\n",
    "#     ('general_encoding', GeneralEncoder()),\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_with_space(loc):\n",
    "    return re.sub(r'([a-z](?=[A-Z])|[A-Z](?=[A-Z][a-z]))', r'\\1 ', loc)\n",
    "\n",
    "def encode_variables(df):\n",
    "    df_ = df.copy()\n",
    "\n",
    "    # Binaries\n",
    "    df_.replace({\"No\":0,\"Yes\":1},inplace=True)\n",
    "\n",
    "    # Wind direction (ordinal)\n",
    "    windDirDic = {\n",
    "        \"E\": 0,\n",
    "        \"ENE\": 1,\n",
    "        \"NE\": 2,\n",
    "        \"NNE\": 3,\n",
    "        \"N\": 4,\n",
    "        \"NNW\": 5,\n",
    "        \"NW\": 6,\n",
    "        \"WNW\": 7,\n",
    "        \"W\": 8,\n",
    "        \"WSW\": 9,\n",
    "        \"SW\": 10,\n",
    "        \"SSW\": 11,\n",
    "        \"S\": 12,\n",
    "        \"SSE\": 13,\n",
    "        \"SE\": 14,\n",
    "        \"ESE\": 15,\n",
    "    }\n",
    "\n",
    "    df_.replace(windDirDic,inplace=True)\n",
    "\n",
    "    # Location\n",
    "    # Get unique locations and get names with spaces\n",
    "    locations = np.array(df_['Location'].unique(),dtype=np.str)\n",
    "    locations = [get_name_with_space(loc) for loc in locations]\n",
    "\n",
    "    # Create location df (name|longitud|latitud|type)\n",
    "    locations_coord = []\n",
    "    geocoder = Nominatim(user_agent = '_')\n",
    "    for loc in locations:\n",
    "        location = geocoder.geocode(loc+' Australia')\n",
    "        if location:\n",
    "            lon=location.longitude\n",
    "            lat=location.latitude\n",
    "            locations_coord.append({\"name\":loc,\"lon\":lon,\"lat\":lat})\n",
    "        else:\n",
    "            raise Exception('no coordinates could be found for '+str(loc))\n",
    "\n",
    "    locations_df = pd.DataFrame.from_dict(locations_coord)\n",
    "    \n",
    "    X = locations_df.drop(['name'],axis=1)    \n",
    "    kmeans = KMeans(n_clusters=6, random_state=0).fit(X)\n",
    "    locations_df[\"type\"] = kmeans.labels_\n",
    "\n",
    "    df_['LocationType'] = df_.apply(\n",
    "        lambda row: int(locations_df.loc[locations_df['name'] == get_name_with_space(row['Location'])].filter(['type']).values[0]), \n",
    "        axis=1)\n",
    "\n",
    "    y = df_.LocationType.values\n",
    "    onehotencoder = OneHotEncoder(categories='auto',sparse=False)\n",
    "    y = onehotencoder.fit_transform(y.reshape(-1,1))\n",
    "    for i in range(y.shape[1]-1):\n",
    "        df_['LocationType_'+str(i)]=y[:,i]\n",
    "\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection / New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_features(df):\n",
    "    df_ = df.copy()\n",
    "\n",
    "    # Pressures\n",
    "    # Mean of colums. When one of the columns is NaN, the mean takes the value of the other column\n",
    "    df_.Pressure3pm.fillna(df_.Pressure9am, inplace=True)\n",
    "    df_.Pressure9am.fillna(df_.Pressure3pm, inplace=True)\n",
    "    press_mean = (df_['Pressure3pm'] + df_['Pressure9am'])/2\n",
    "    df_['PressureMean'] = press_mean\n",
    "    df_.drop('Pressure3pm', inplace=True, axis=1)\n",
    "    df_.drop('Pressure9am', inplace=True, axis=1)\n",
    "\n",
    "    # Temperatures\n",
    "    temp_diff = df_['Temp3pm'] - df_['Temp9am']\n",
    "    df_['TempDiff'] = temp_diff\n",
    "    # Mean of colums. When one of the columns is NaN, the mean takes the value of the other column\n",
    "    df_.Temp3pm.fillna(df_.Temp9am, inplace=True)\n",
    "    df_.Temp9am.fillna(df_.Temp3pm, inplace=True)\n",
    "    temp_mean = (df_['Temp3pm'] + df_['Temp9am'])/2\n",
    "    df_['TempMean'] = temp_mean\n",
    "    df_.drop('Temp3pm', inplace=True, axis=1)\n",
    "    df_.drop('Temp9am', inplace=True, axis=1)\n",
    "\n",
    "    # Temperatures max-min\n",
    "    max_temp_diff = df_['MaxTemp'] - df_['MinTemp']\n",
    "    df_['TempMaxDiff'] = max_temp_diff\n",
    "    df_.drop('MinTemp', inplace=True, axis=1)\n",
    "    df_.drop('MaxTemp', inplace=True, axis=1)\n",
    "\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_skewed_boundaries(df, variable, distance=1.5):\n",
    "    IQR = df[variable].quantile(0.75) - df[variable].quantile(0.25)\n",
    "    lower_boundary = df[variable].quantile(0.25) - (IQR * distance)\n",
    "    upper_boundary = df[variable].quantile(0.75) + (IQR * distance)\n",
    "    return upper_boundary, lower_boundary\n",
    "    \n",
    "def transform_outliers(df, norm_col, threshold_capped=1.5, threshold_trimmed=1.8,  test_mode=False, limits={},\n",
    "                        use_manual_limits=False, upper_limit_trim=0, lower_limit_trim=0, upper_limit_cap=0, lower_limit_cap=0, print_output=False):\n",
    "    \n",
    "    outliers_limits={}\n",
    "    # Trimming and capping outliers\n",
    "    outliers_total = np.array(np.repeat(False,df.shape[0]))\n",
    "    df_capped = df.copy()\n",
    "    for col in norm_col:\n",
    "        \n",
    "        if not test_mode:\n",
    "            outliers_limits_col={}\n",
    "            if use_manual_limits:\n",
    "                upper_limit, lower_limit = upper_limit_trim, lower_limit_trim\n",
    "            else:\n",
    "                upper_limit, lower_limit = find_skewed_boundaries(df, col, threshold_trimmed)\n",
    "\n",
    "            outliers_limits_col['upper_limit_trim'] = upper_limit\n",
    "            outliers_limits_col['lower_limit_trim'] = lower_limit\n",
    "        else:\n",
    "            upper_limit, lower_limit = limits[col]['upper_limit_trim'], limits[col]['lower_limit_trim']\n",
    "\n",
    "        outliers = np.where(df[col] > upper_limit, True,\n",
    "                            np.where(df[col] < lower_limit, True, False))                        \n",
    "        outliers_total = np.logical_or(outliers_total, outliers)\n",
    "        \n",
    "        if print_output:\n",
    "            print(str(col) + \" outliers = \"+str(outliers.sum()))\n",
    "        \n",
    "        if not test_mode:\n",
    "            if use_manual_limits:\n",
    "                upper_limit, lower_limit = upper_limit_cap, lower_limit_cap\n",
    "            else:\n",
    "                upper_limit, lower_limit = find_skewed_boundaries(df, col, threshold_capped)\n",
    "\n",
    "            outliers_limits_col['upper_limit_cap'] = upper_limit\n",
    "            outliers_limits_col['lower_limit_cap'] = lower_limit\n",
    "            outliers_limits[col] = outliers_limits_col\n",
    "        else:\n",
    "            upper_limit, lower_limit = limits[col]['upper_limit_cap'], limits[col]['lower_limit_cap']\n",
    "\n",
    "        df_capped[col] = np.where(df[col] > upper_limit, upper_limit,\n",
    "                            np.where(df[col] < lower_limit, lower_limit, df_capped[col]))\n",
    "\n",
    "    if print_output:\n",
    "        print(\"Total outliers = \"+str(outliers_total.sum()))\n",
    "        \n",
    "    df_trimmed = df_capped.loc[~(outliers_total)]\n",
    "    \n",
    "    if not test_mode:\n",
    "        return df_trimmed, outliers_limits\n",
    "    else:\n",
    "        return df_trimmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple mean imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_imputer(df,cols,test_mode=False,imputer_list=[]):\n",
    "    df_imputed = df.copy()\n",
    "    df_imputed['imputed_mean'] = 0\n",
    "    mean_imputer_list = []\n",
    "\n",
    "    for k in np.sort(pd.unique(df['LocationType'])):        \n",
    "        # print(\"Running mean_imputer for cluster: \"+str(k))\n",
    "\n",
    "        indx=df.loc[df['LocationType'] == k][cols].copy().index # Get index\n",
    "        \n",
    "        if not test_mode:\n",
    "            imputer = SimpleImputer(strategy='mean',add_indicator=True)\n",
    "        else:\n",
    "            imputer = imputer_list[k]\n",
    "\n",
    "        mapper = DataFrameMapper([(cols, imputer)])\n",
    "        \n",
    "        # df.loc[df['LocationType'] == k][cols]\n",
    "        transform_features = mapper.fit_transform(df.loc[df['LocationType'] == k][cols].copy(), 4) # Round 4 digits\n",
    "        qty_imputed_cols = np.array(transform_features).shape[1]-len(cols) # Number of imputed columns\n",
    "        imputed_by_mean_imp_col = [str(d) + \"_imputed\" for d in range(qty_imputed_cols)] # Name of indicators of imputation\n",
    "        transform_features_df = pd.DataFrame(transform_features, index=indx, columns=cols+imputed_by_mean_imp_col)\n",
    "\n",
    "        # Add column of imputed indicator. Only one column as an \"or\" logical operation of all indicators.\n",
    "        transform_features_df['imputed_mean'] = (transform_features_df[imputed_by_mean_imp_col].sum(axis=1)>0).astype(int)\n",
    "        df_imputed.loc[indx,cols+['imputed_mean']]=transform_features_df[cols+['imputed_mean']] # Replace in original dataset\n",
    "\n",
    "        # Check data\n",
    "        assert(not np.any(df_imputed[df_imputed.LocationType==k][cols].isna().sum()>0))\n",
    "\n",
    "        mean_imputer_list.append(imputer)\n",
    "\n",
    "    # Round discrete columns    \n",
    "    df_imputed.WindDir3pm = df_imputed.WindDir3pm.round()\n",
    "    \n",
    "    if not test_mode:\n",
    "        return df_imputed, mean_imputer_list\n",
    "    else:\n",
    "        return df_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_imputer(df, cols, neighbors=5,test_mode=False,imputer_list=[]):\n",
    "    df_imputed = df.copy()\n",
    "    df_imputed['imputed_knn'] = 0\n",
    "    knn_imputer_list = []\n",
    "    \n",
    "    for k in np.sort(pd.unique(df['LocationType'])):\n",
    "        # print(\"Running knn_imputer for cluster: \"+str(k))\n",
    "\n",
    "        indx=df.loc[df['LocationType'] == k][cols].copy().index # Get index\n",
    "        \n",
    "        if not test_mode:            \n",
    "            imputer = KNNImputer(n_neighbors=neighbors,add_indicator=True)\n",
    "        else:\n",
    "            imputer = imputer_list[k]            \n",
    "        mapper = DataFrameMapper([(cols, imputer)])\n",
    "\n",
    "        # df.loc[df['LocationType'] == k][cols+['LocationType']]\n",
    "        transform_features = mapper.fit_transform(df.loc[df['LocationType'] == k][cols].copy(), 4) # Round 4 digits\n",
    "        qty_imputed_cols = np.array(transform_features).shape[1]-len(cols) # Number of imputed columns\n",
    "        imputed_by_knn_imp_col = [str(d) + \"_imputed\" for d in range(qty_imputed_cols)] # Name of indicators of imputation\n",
    "        transform_features_df = pd.DataFrame(transform_features, index=indx, columns=cols+imputed_by_knn_imp_col)\n",
    "        transform_features_df.isna().sum()\n",
    "\n",
    "        # Add column of imputed indicator. Only one column as an \"or\" logical operation of all indicators.\n",
    "        transform_features_df['imputed_knn'] = (transform_features_df[imputed_by_knn_imp_col].sum(axis=1)>0).astype(int)\n",
    "        df_imputed.loc[indx,cols+['imputed_knn']]=transform_features_df[cols+['imputed_knn']] # Replace in original dataset\n",
    "\n",
    "        # Check data\n",
    "        assert(not np.any(df_imputed[df_imputed.LocationType==k][cols].isna().sum()>0))\n",
    "        \n",
    "        knn_imputer_list.append(imputer)\n",
    "\n",
    "    # Round discrete columns   \n",
    "    df_imputed.RainToday = df_imputed.RainToday.round()\n",
    "    df_imputed.WindDir9am = df_imputed.WindDir9am.round()\n",
    "    df_imputed.WindGustDir = df_imputed.WindGustDir.round()\n",
    "    if not test_mode:\n",
    "        return df_imputed, knn_imputer_list\n",
    "    else:\n",
    "        return df_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclic encoding for wind speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclic_encoding(df, cols, cat):\n",
    "    '''\n",
    "    Number of different categories\n",
    "    '''\n",
    "    df_ = df.copy()\n",
    "    for c in cols:\n",
    "        df_[c+'_cos'] = np.cos(2 * np.pi * (df_[c]/cat))\n",
    "        df_[c+'_sin'] = np.sin(2 * np.pi * (df_[c]/cat))\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('weatherAUS.csv')\n",
    "\n",
    "# Add RainfallTomorrow and RainfallYesterday and delete the first and the last element of each city\n",
    "df['RainfallTomorrow']=df['Rainfall'].shift(-1)\n",
    "df['RainfallYesterday']=df['Rainfall'].shift(1)\n",
    "\n",
    "remove_index = []\n",
    "for l in pd.unique(df['Location']):\n",
    "    remove_index.append(df.index[df['Location'] == l][0])\n",
    "    remove_index.append(df.index[df['Location'] == l][-1])\n",
    "\n",
    "df.drop(df.index[np.array(remove_index)],inplace=True)\n",
    "\n",
    "# Drop columns where RainTomorrow is NaN\n",
    "df = df[df['RainTomorrow'].notna()]\n",
    "\n",
    "# Split into train/test\n",
    "X_train_, X_test_, y_train_, y_test_ = train_test_split(\n",
    "    df.drop(['RainTomorrow'], axis=1),\n",
    "    df['RainTomorrow'],\n",
    "    test_size=0.15,\n",
    "    random_state=0)\n",
    "\n",
    "# Create df_train and df_test to process all columns together\n",
    "df_train = X_train_.copy()\n",
    "df_train['RainTomorrow']=y_train_\n",
    "\n",
    "df_test = X_test_.copy()\n",
    "df_test['RainTomorrow']=y_test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_df(df, test_mode=False, param_dict={}):\n",
    "\n",
    "    if test_mode and not param_dict:\n",
    "        raise Exception('param_list need for test mode')\n",
    "\n",
    "    df_ = df.copy()\n",
    "\n",
    "    # Drop columns\n",
    "    drop_first_cols=['Evaporation','Sunshine','Cloud9am','Cloud3pm','Date']\n",
    "    df_ = drop_cols(df_,drop_first_cols)\n",
    "\n",
    "    # Variable Encoding\n",
    "    df_ = encode_variables(df_)\n",
    "\n",
    "    # Create/change some features\n",
    "    df_ = new_features(df_)\n",
    "\n",
    "    # Outliers\n",
    "    norm_col = ['WindGustSpeed','WindSpeed9am','WindSpeed3pm','Humidity9am',\n",
    "                'Humidity3pm','PressureMean','TempDiff','TempMean','TempMaxDiff']\n",
    "\n",
    "    norm_rainfall = ['Rainfall','RainfallYesterday','RainfallTomorrow']\n",
    "\n",
    "    if not test_mode:\n",
    "        df_, outliers_limits_ = transform_outliers(df_, norm_col)\n",
    "        df_, outliers_limits_rainfall_ = transform_outliers(df_, norm_rainfall,use_manual_limits=True, \n",
    "                                        upper_limit_trim=15, lower_limit_trim=0, upper_limit_cap=10, lower_limit_cap=0)\n",
    "    else:\n",
    "        df_ = transform_outliers(df_, norm_col, test_mode=test_mode, limits=param_dict['outliers_limits'])\n",
    "        df_ = transform_outliers(df_, norm_rainfall, test_mode=test_mode, limits=param_dict['outliers_limits_rainfall'])\n",
    "    \n",
    "    # Scaling\n",
    "    scaled_columns = ['Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm','Humidity9am','Humidity3pm',\n",
    "                    'RainfallYesterday','RainfallTomorrow','PressureMean','TempDiff','TempMean','TempMaxDiff']\n",
    "\n",
    "    if not test_mode:\n",
    "        scaler_ = StandardScaler()\n",
    "        scaler_.fit(df_[scaled_columns])\n",
    "    else:\n",
    "        scaler_ = param_dict['scaler']\n",
    "        \n",
    "    df_[scaled_columns] = scaler_.transform(df_[scaled_columns])\n",
    "\n",
    "    # Imputation\n",
    "    imputed_by_mean_col = ['Rainfall','WindSpeed9am','WindSpeed3pm','Humidity9am','Humidity3pm',\n",
    "                        'RainfallYesterday','RainfallTomorrow','TempDiff','TempMean','TempMaxDiff','WindDir3pm']\n",
    "    imputed_by_knn_col = ['RainToday','WindGustDir','WindGustSpeed','WindDir9am','PressureMean']\n",
    "    neighbors_col =['Rainfall','WindDir3pm','WindSpeed9am','WindSpeed3pm','Humidity9am','Humidity3pm','TempDiff','TempMean','TempMaxDiff']\n",
    "\n",
    "    if not test_mode:\n",
    "        df_, mean_imputer_list_ = mean_imputer(df_,imputed_by_mean_col)\n",
    "        df_, knn_imputer_list_ = knn_imputer(df_,imputed_by_knn_col+neighbors_col)\n",
    "    else:\n",
    "        df_ = mean_imputer(df_, imputed_by_mean_col, test_mode=test_mode, imputer_list=param_dict['mean_imputer_list'])\n",
    "        df_ = knn_imputer(df_, imputed_by_knn_col+neighbors_col, test_mode=test_mode, imputer_list=param_dict['knn_imputer_list'])\n",
    "\n",
    "    # Cyclic encoding for wind speed\n",
    "    wind_dir_colums = ['WindGustDir','WindDir9am','WindDir3pm']\n",
    "    df_ = cyclic_encoding(df_,cols=wind_dir_colums,cat=16)\n",
    "\n",
    "    # Normalization\n",
    "    norm_col_yj = ['Humidity9am','Humidity3pm','PressureMean','TempDiff','TempMean','TempMaxDiff']\n",
    "    norm_col_qt = ['WindGustSpeed','WindSpeed9am','WindSpeed3pm']\n",
    "    \n",
    "    if not test_mode:\n",
    "        power_yj_ = PowerTransformer(method= 'yeo-johnson')\n",
    "        qt_transf_ = QuantileTransformer(output_distribution='normal')\n",
    "        power_yj_.fit(df_[norm_col_yj])\n",
    "        qt_transf_.fit(df_[norm_col_qt])\n",
    "    else:\n",
    "        power_yj_ = param_dict['power_yj']\n",
    "        qt_transf_ = param_dict['qt_transf']\n",
    "                \n",
    "    df_[norm_col_yj] = power_yj_.transform(df_[norm_col_yj])\n",
    "    df_[norm_col_qt] = qt_transf_.transform(df_[norm_col_qt])\n",
    "\n",
    "    # Drop columns\n",
    "    drop_columns = ['Location','LocationType','WindGustDir','WindDir9am','WindDir3pm']\n",
    "    df_.drop(drop_columns,axis=1,inplace=True)\n",
    "    if not test_mode:\n",
    "        param_dict={'outliers_limits': outliers_limits_, \n",
    "                'outliers_limits_rainfall': outliers_limits_rainfall_, \n",
    "                'scaler': scaler_, \n",
    "                'mean_imputer_list': mean_imputer_list_, \n",
    "                'knn_imputer_list': knn_imputer_list_, \n",
    "                'power_yj': power_yj_, \n",
    "                'qt_transf': qt_transf_\n",
    "        }\n",
    "        return df_, param_dict\n",
    "\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_processed, param_dict = pre_process_df(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_processed = pre_process_df(df_test, test_mode=True, param_dict=param_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data into pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_processed.to_pickle('./data/df_train_processed.pkl')\n",
    "\n",
    "# open_file = open('./data/param_dict.pkl', \"wb\")\n",
    "# pickle.dump(param_dict, open_file)\n",
    "# open_file.close()\n",
    "\n",
    "# df_test_processed.to_pickle('./data/df_test_processed.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_processed = pd.read_pickle('./data/df_train_processed.pkl')\n",
    "\n",
    "# open_file = open('./data/param_dict.pkl', \"rb\")\n",
    "# param_dict = pickle.load(open_file)\n",
    "# open_file.close()\n",
    "\n",
    "# df_test_processed = pd.read_pickle('./data/df_test_processed.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = df_train_processed.drop(['RainTomorrow','RainfallTomorrow'],axis=1)\n",
    "# y_train = df_train_processed.RainTomorrow\n",
    "\n",
    "# X_test = df_test_processed.drop(['RainTomorrow','RainfallTomorrow'],axis=1)\n",
    "# y_test = df_test_processed.RainTomorrow\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(\n",
    "    df_train_processed.drop(['RainTomorrow','RainfallTomorrow'], axis=1),\n",
    "    df_train_processed['RainTomorrow'],\n",
    "    test_size=0.2,\n",
    "    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "explained_variance = .95\n",
    "pca = PCA(n_components=explained_variance).fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_cv_pca = pca.transform(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7516023358495941\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# log_reg4=LogisticRegression(C=1.0, class_weight='balanced', penalty='l2',\n",
    "#                    random_state=None)\n",
    "\n",
    "logisticRegr = LogisticRegression(class_weight='balanced')\n",
    "logisticRegr.fit(X_train_pca, y_train)\n",
    "\n",
    "# Prediction\n",
    "predictions = logisticRegr.predict(X_cv_pca)\n",
    "\n",
    "# Use score method to get accuracy of model\n",
    "score = logisticRegr.score(X_cv_pca, y_cv)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.38043635312816065\n",
      "accuracy = 0.7516023358495941\n",
      "recall = 0.7360916969527537\n",
      "f1_score = 0.5016193560678224\n",
      "\n",
      "confusion matrix:\n",
      "[[13198  4288]\n",
      " [  944  2633]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"precision = \"+str(metrics.precision_score(y_cv, predictions)))\n",
    "print(\"accuracy = \"+str(metrics.accuracy_score(y_cv, predictions)))\n",
    "print(\"recall = \"+str(metrics.recall_score(y_cv, predictions)))\n",
    "print(\"f1_score = \"+str(metrics.f1_score(y_cv, predictions)))\n",
    "print(\"\\nconfusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_cv, predictions))\n",
    "# print(metrics.classification_report(y_cv, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
