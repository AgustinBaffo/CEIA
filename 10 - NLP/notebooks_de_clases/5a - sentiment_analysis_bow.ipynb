{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5a - sentiment_analysis_bow.ipynb","provenance":[{"file_id":"1DcWKK2rk8E_BhVkmIA3I3mlOdDZHX0fs","timestamp":1648935768274}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kBLpTr7plguX"},"source":["<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Procesamiento de lenguaje natural\n","## Sentiment analysis con Bag of Words"]},{"cell_type":"markdown","metadata":{"id":"9W6nuajhlqZD"},"source":["### Objetivo\n","El objetivo es utilizar las críticas de películas para que el sistema determine si la evaluación es positiva o negativa (sentiment analysis como clasificador binario de texto)"]},{"cell_type":"code","source":["!pip install --upgrade --no-cache-dir gdown --quiet"],"metadata":{"id":"-gi_-4bnZab6"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hCpOVzJdl8_p"},"source":["import numpy as np\n","import random\n","import io\n","import pickle\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oDbSydDfza5u"},"source":["from sklearn.metrics import roc_curve\n","from sklearn.metrics import auc\n","\n","def curva_roc(model, X_test, y_test):\n","    y_hat_prob = model.predict(X_test).ravel()\n","    #y_hat = [1 if x >= 0.5 else 0 for x in y_hat_prob]\n","    mask_positive = y_hat_prob >= 0.5\n","    mask_negative = y_hat_prob < 0.5\n","    y_hat = y_hat_prob\n","    y_hat[mask_positive] = 1\n","    y_hat[mask_negative] = 0\n","    y_hat = y_hat.astype(int)\n","\n","    # Calcular la exactitud (accuracy)\n","    scores = model.evaluate(X_test, y_test)\n","    print(\"Accuracy:\", scores[1])\n","\n","    fpr, tpr, thresholds = roc_curve(y_test, y_hat_prob)\n","    auc_keras = auc(fpr, tpr)\n","    print('auc_keras', auc_keras)\n","\n","    plt.figure(1)\n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.plot(fpr, tpr, label='Keras (area = {:.3f})'.format(auc_keras))\n","    plt.xlabel('False positive rate')\n","    plt.ylabel('True positive rate')\n","    plt.title('Curva ROC test')\n","    plt.legend(loc='best')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8UPeRkrAmbF3"},"source":["### Datos\n","Utilizaremos como dataset críticas de películas de IMDB puntuadas deforma positiva o negativa.\\\n","Referencia del dataset: [LINK](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)"]},{"cell_type":"code","metadata":{"id":"C7jLvTU3lSyL"},"source":["# Descargar la carpeta de dataset\n","import os\n","import gdown\n","if os.access('imdb_dataset.csv', os.F_OK) is False:\n","    url = 'https://drive.google.com/uc?id=1fXW-u9NVbH1yhwU1AHzPVtgGyV1c8N3g'\n","    output = 'imdb_dataset.csv'\n","    gdown.download(url, output, quiet=False)\n","else:\n","    print(\"El dataset ya se encuentra descargado\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o-SV1P3dnD1J"},"source":["# Armar el dataset\n","df = pd.read_csv('imdb_dataset.csv')\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q-OwSePKm-FK"},"source":["### 1 - Limpieza de datos\n","- En los datos se observo que en la columna \"review\" hay código HTML de salto de línea.\n","- Tranformar la columna snetiment a 0 y 1\n","\n"]},{"cell_type":"code","metadata":{"id":"-hc7-AmYnPC3"},"source":["# En los datos se observó código de HTML de salto de línea <br />\n","import re\n","df_reviews = df.copy() \n","df_reviews['review'] = df['review'].apply(lambda x: re.sub(\"<br />\", \"\", x))\n","df_reviews['sentiment'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).values\n","df_reviews.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sZtvASVOn3ty"},"source":["# Observar como está distribuido el dataset respecto a la columna Rating\n","# es decir, observar que tan balanceado se encuentra respecot a cada clase\n","df_reviews['sentiment'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v7QJ2poZn9b-"},"source":["# Observar como está distribuido el dataset\n","sns.countplot(x='sentiment', data=df_reviews)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"juVSYR89x_2v"},"source":["Se puede observar que el dataset está perfectamente balanceado"]},{"cell_type":"code","metadata":{"id":"gVJ_RVi4o1h3"},"source":["# Tomar la columna de las review y almacenarlo todo en un vector numpy de reviews\n","text_sequences = df_reviews['review'].values\n","text_sequences.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4nT5Un_co65Q"},"source":["# Cuantas reviews (rows) hay para evaluar?\n","len(text_sequences)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2F8hqqw05WSJ"},"source":["# - Por defecto CountVectorizer elimina los signos de puntuacion y transforma\n","# todas las palabras a lowercase\n","# - max_features --> limitacion la máxima dimensión del oneHotEncoding (max vocab_size)\n","# - stop_words --> quitamos aquellas palabras que para el idioma no se consideran\n","# relevantes (como los árticulos, pronombres, preposiciones, adverbios, etc)\n","# - Referencia:\n","# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n","from sklearn.feature_extraction.text import CountVectorizer\n","vectorizer = CountVectorizer(max_features=2000, stop_words='english')\n","X = vectorizer.fit_transform(text_sequences).toarray()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zp5bvozW5p3o"},"source":["# Los datos de entrada (X) son un vector de oneHotEncoding del tamaño\n","# del vocabulario y de la cantidad de filas\n","X.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"llVM-tzQo9_F"},"source":["# Tomar la columna rating y alcemacenarla en una variable \"y\"\n","# Su shape debe ser equivalente la cantidad de rows del corpus\n","y = df_reviews['sentiment'].values\n","print(y.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rmz9A6n4uK4V"},"source":["# Dividir los datos en train y test\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EcDPlhEouQ9E"},"source":["# Determinar la dimensiones de entrada y salida\n","in_shape = X_train.shape[1] # max input sentence len\n","out_shape = 1 # binary classification\n","print(\"in_shape\", in_shape, \", out_shape\", out_shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NpbQHExL6OTu"},"source":["### 2 - Entrenar el modelo DNN con BOW"]},{"cell_type":"code","metadata":{"id":"NUkuWBsM6cx3"},"source":["from keras.models import Sequential\n","from keras.layers import Dense, Dropout\n","\n","# Armar un modelo de clasificacion binaria con DNN\n","model = Sequential()\n","model.add(Dense(units=128, activation='relu', input_shape=(in_shape,)))\n","model.add(Dropout(rate=0.3))\n","model.add(Dense(units=64, activation='relu'))\n","model.add(Dropout(rate=0.5))\n","model.add(Dense(units=32, activation='relu'))\n","model.add(Dropout(rate=0.5))\n","model.add(Dense(units=out_shape, activation='sigmoid'))\n","\n","model.compile(optimizer=\"Adam\",\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IiHo33J8opab"},"source":["from keras.utils.vis_utils import plot_model\n","plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_oTSAIjeo73-"},"source":["hist = model.fit(X_train, y_train, epochs=20, validation_split=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y-cjIatVpPqW"},"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","epoch_count = range(1, len(hist.history['accuracy']) + 1)\n","sns.lineplot(x=epoch_count,  y=hist.history['accuracy'], label='train')\n","sns.lineplot(x=epoch_count,  y=hist.history['val_accuracy'], label='valid')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jx1tLx23pbRi"},"source":["model.evaluate(X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yr78NmybzPMP"},"source":["# Como este modelo es binario podemos calcular la curva ROC\n","curva_roc(model, X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_2_73kr1DDDS"},"source":["### 3 - Entrenar un modelo previamente reduciendo el vector de entrada (X)"]},{"cell_type":"code","metadata":{"id":"HP5uN9tqpHu_"},"source":["# Un vector de 2000 columnas es demasiado grande para entrenar un modelo clásico\n","# de deep learning (DNN)\n","# Se utiliza PCA para reducir la dimensionalidad\n","from sklearn.decomposition import PCA\n","X_pca = PCA(n_components=50).fit_transform(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xGsHjJMD639r"},"source":["X_pca.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rcqMV9hiDQBu"},"source":["# Dividir los datos en train y test\n","from sklearn.model_selection import train_test_split\n","X_train2, X_test2, y_train2, y_test2 = train_test_split(X_pca, y, test_size=0.2, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k7Leb-GrDahe"},"source":["# Determinar la dimensiones de entrada y salida\n","in_shape = X_train2.shape[1] # max input sentence len\n","out_shape = 1 # binary classification\n","print(\"in_shape\", in_shape, \", out_shape\", out_shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jMVCSYClDevO"},"source":["from keras.models import Sequential\n","from keras.layers import Dense, Dropout\n","\n","# Utilizar la misma estructura de modelo del punto anterior\n","model2 = Sequential()\n","model2.add(Dense(units=128, activation='relu', input_shape=(in_shape,)))\n","model2.add(Dropout(rate=0.3))\n","model2.add(Dense(units=64, activation='relu'))\n","model2.add(Dropout(rate=0.5))\n","model2.add(Dense(units=32, activation='relu'))\n","model2.add(Dropout(rate=0.5))\n","model2.add(Dense(units=out_shape, activation='sigmoid'))\n","\n","model2.compile(optimizer=\"Adam\",\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","model2.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tM1exaqQDp4U"},"source":["from keras.utils.vis_utils import plot_model\n","plot_model(model2, to_file='model2_plot.png', show_shapes=True, show_layer_names=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g4PsbnyDDSdz"},"source":["hist2 = model2.fit(X_train2, y_train2, epochs=20, validation_split=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DOHxGyacDz6f"},"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","epoch_count = range(1, len(hist2.history['accuracy']) + 1)\n","sns.lineplot(x=epoch_count,  y=hist2.history['accuracy'], label='train')\n","sns.lineplot(x=epoch_count,  y=hist2.history['val_accuracy'], label='valid')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CSMgxRyND4CP"},"source":["model2.evaluate(X_test2, y_test2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aEqQbY5YECTk"},"source":["# Como este modelo es binario podemos calcular la curva ROC\n","curva_roc(model2, X_test2, y_test2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fGDesEf7sNrP"},"source":["### 4 - Conclusión\n","El modelo con \"bag of words\" resultó ser muy fácil de armar, barato de entrenar (liviano) y obtuvo una muy buena performance.\\\n","El modelo de entrada completa (oneHotEncoding) performó mejor que el dimensión reducida con PCA pero realizó mucho overfitting."]}]}